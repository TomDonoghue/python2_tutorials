{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing / Scraping Web Data With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a brief overview of scraping web data, parsing it, and doing rudimentary analyses. \n",
    "It's focused on pulling text data.\n",
    "\n",
    "Note: This quick overview does assume a fair amount of prior knowledge. In particular:\n",
    "- HTML knowledge. This assumes some level of awareness of how webpages work and are organized. \n",
    "    - If you want to know a bit about this, check out: http://www.htmldog.com\n",
    "- Python knowledge. Here, I assume some awareness of basic Python / programming\n",
    "    - If you want to learn more about Python, try the 'Python_Basic' or 'Python_Advanced' notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Web Data Packages\n",
    "import lxml                        # LMXL is a package for processing HTML/XML data - http://lxml.de\n",
    "import requests                    # Requests is a package for getting webpage data - http://docs.python-requests.org/\n",
    "from bs4 import BeautifulSoup      # BeautifulSoup is a package for pulling data out of HTML/XML \n",
    "                                   #    http://www.crummy.com/software/BeautifulSoup \n",
    "\n",
    "# Import a function from natural language toolkit to use on our web-scraped text\n",
    "# This function will be useful, but not explored, as it is not the focus of this Notebook\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab a webpage. The requests \n",
    "# Let's analyze the Beautiful Soup Documentation Page\n",
    "#webpage = requests.get(\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse the webpage\n",
    "webpage_soup = BeautifulSoup(webpage.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title id=\"pageTitle\">Facebook - Log In or Sign Up</title>\n"
     ]
    }
   ],
   "source": [
    "# We can pull things out of the webpage, for example the title, \n",
    "print webpage_soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genindex.html\n",
      "#\n",
      "#beautiful-soup-documentation\n",
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "#porting-code-to-bs4\n"
     ]
    }
   ],
   "source": [
    "# We can search through different tags on the website, and see what they contain.\n",
    "# For example, we can collect a list of all the links used on the website.\n",
    "list_of_links = list()\n",
    "for link in webpage_soup.find_all('a'):\n",
    "    list_of_links.append(link.get('href'))\n",
    "    \n",
    "# Print the first few entries in our list of links\n",
    "for i in range(0,6):              # We could also do 'print list_of_refs[0:6]', but that wouldn't add new lines between entries\n",
    "    print list_of_links[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "http://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\n",
      "http://kondou.com/BS4/\n",
      "http://coreapython.hosting.paran.com/etc/beautifulsoup4.html\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "http://www.crummy.com/software/BeautifulSoup/download/4.x/\n",
      "http://lxml.de/\n",
      "http://code.google.com/p/html5lib/\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n"
     ]
    }
   ],
   "source": [
    "# Notice that these links include internal and external links. \n",
    "#    '# something' - is a link to elsewhere on the same HTML webpage\n",
    "#    'http:...'    - is a link to a different HTML webpage (to a different website)\n",
    "\n",
    "# Let's only print out external links\n",
    "for link in webpage_soup.find_all('a'):\n",
    "    if 'http' in link.get('href'):\n",
    "        print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of our stored text is:  <type 'unicode'>\n",
      "Number of characters in our text is:  68252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u\"\\n\\n\\nBeautiful Soup Documentation \\u2014 Beautiful Soup 4.4.0 documentation\\n\\n\\n\\n      var DOCUMENTATION_OPTIONS = {\\n        URL_ROOT:    './',\\n        VERSION:     '4.4.0',\\n        COLLAPSE_INDEX: false,\\n    \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also try to look at the text that is on a webpage\n",
    "# There is the '.get_text()' command that is useful for this\n",
    "webpage_text = webpage_soup.get_text()\n",
    "\n",
    "# 'get_text' returns unicode for all the text it found. This could be (and is in this case) a lot of text.\n",
    "print 'Data type of our stored text is: ', type(webpage_text)\n",
    "print 'Number of characters in our text is: ', len(webpage_text) \n",
    "\n",
    "# Let's have a peak at what our text looks like. Let's print the first couple hundred characters.\n",
    "webpage_text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of all_text is:  <type 'unicode'>\n",
      "Length of all text is:  36180\n",
      "\n",
      "Beautiful Soup is a\n",
      "Python library for pulling data out of HTML and XML files. It works\n",
      "with your favorite parser to provide idiomatic ways of navigating,\n",
      "searching, and modifying the parse tree. It c\n"
     ]
    }
   ],
   "source": [
    "# Notice above that 'get_text()' called on the whole webpage gets a lot of text we don't want. \n",
    "# To try to focus in on the text we want, let's focus in on 'p' tags in the HTML\n",
    "\n",
    "# Get a list of all the paragraph tags in our web page\n",
    "webpage_paragraphs = webpage_soup.find_all('p')\n",
    "\n",
    "# Loop through all the paragraphs, collecting the text contained inside them. \n",
    "all_text = ''                              # Initialize a variable to collect all the text in\n",
    "for p in webpage_paragraphs:\n",
    "    all_text = all_text + p.get_text()\n",
    "  \n",
    "# Now let's again look at what we have. Let's print the type, length, and a couple hundred characters of what we pulled out.\n",
    "print 'Type of all_text is: ', type(all_text)\n",
    "print 'Length of all text is: ', len(all_text)\n",
    "print ''\n",
    "print all_text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[u'Beautiful', u'Soup', u'is', u'a', u'Python', u'library', u'for', u'pulling', u'data', u'out', u'of', u'HTML', u'and', u'XML', u'files.', u'It', u'works', u'with', u'your', u'favorite', u'parser', u'to', u'provide', u'idiomatic', u'ways', u'of', u'navigating,', u'searching,', u'and', u'modifying']\n"
     ]
    }
   ],
   "source": [
    "# Again, we have a long piece of unicode, but what it contains looks a lot more like what we want!\n",
    "\n",
    "# Notice that right now we just have continuous unicode. Really we're interested in words. \n",
    "# It would be much easier to work with a list of words. \n",
    "\n",
    "# Let's split up the unicode variable we have into words\n",
    "words = all_text.split()\n",
    "\n",
    "# Let's have a look at what we have\n",
    "print type(words)\n",
    "print words[0:30]\n",
    "\n",
    "# Note that we have a list of words. The 'u' before each item means they are unicode variables. That is fine for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 398), (u'a', 229), (u'of', 131), (u'you', 117), (u'to', 106)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now start analyzing our extracted data!\n",
    "# Let's try by simply looking for the most frequency words on this webpage. \n",
    "# We will use the FreqDist function from nltk to do this. \n",
    "\n",
    "# FreqDist calculates the frequency distribution for all the words we give it. \n",
    "word_freq = FreqDist(words)\n",
    "# most_common will plot the n most common words\n",
    "word_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unsurprisingly and uninterestingly, the most common words on this webpage are words like 'the' and 'a'. \n",
    "# That doesn't actually tell us very much. Let's try to get rid of those words to see the more interesting words.\n",
    "\n",
    "# First let's make a list of words we don't think are interesting\n",
    "bad_words = ('a', 'by', 'from', 'which', 'such', 'has', 'also', 'to', 'and', 'be', 'into', 'the', 'you', 'will', \n",
    "             'this', 'have', 'use', 'of', 'is', 'in', 'that', 'can', 'it', 'or', 'as', 'for', 'an', 'but', 'not', \n",
    "             'all', 'on', 'are', 'with','at', 'was','its', 'than', 'been', 'used', 'using', 'through')\n",
    "\n",
    "# Now, let's loop through our list of words, only keeping the words if they are not in our list of bad words. \n",
    "words_cleaned = list()\n",
    "for word in words:\n",
    "    # If the word is not in our list of bad words, we add it to our new list of good words\n",
    "    # Note that we are forcing all words to be all lowercase (with '.lower())\n",
    "    if word.lower() not in bad_words:\n",
    "        words_cleaned.append(word.lower())\n",
    "\n",
    "# NOTE: usually, you will not make your own list of bad words, but rather use tools like nltk to do this for you. \n",
    "# Check out the notebook on 'Text Mining', which covers nltk, for more information about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Soup', 106),\n",
       " (u'Beautiful', 104),\n",
       " (u'tag', 89),\n",
       " (u'document', 71),\n",
       " (u'string', 48)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets look at which words are the most common in the our cleaned scraped data\n",
    "word_freq = FreqDist(words_cleaned)\n",
    "word_freq.most_common(5)\n",
    "\n",
    "# This is a bit more interesting. \n",
    "# The most common words are the package names, and related words like 'tag', 'document' and 'string'\n",
    "# There is a ton more we can do with this, but that's text mining - not the main topic here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigger Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we have the tools to do some kinds of basic analysis of web data, that could be informative. \n",
    "\n",
    "If I had some other information, such as website traffic data, or user ratings, I could already ask questions like:\n",
    "- Does the number of internal links correlate with traffic? With user ratings? What about the number of external links?\n",
    "\n",
    "This would require searching through multiple webpages. We could try and analyze package tutorial webpages, and try to \n",
    "see if we can predict something about their use (traffic or ratings) from their composition. \n",
    "\n",
    "So far we have looked \n",
    "\n",
    "More likely, we want to do some sort of scraping - such as pulling data off multiple webpages for some kind of further analysis. \n",
    "Let's look at something like that. \n",
    "\n",
    "Let's say I want to analyze something about the way different programming languages are discussed online. \n",
    "\n",
    "I decide on some subset of languages, and decide I'm going to do a pilot study only looking at Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up things for web scraping wikipedia pages for programming languages\n",
    "\n",
    "# URL for wikipedia pages\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "# A dictionary including languages of interest (python, matlab, R, etc.) and the URL extension for their wiki page\n",
    "languages = {'Python': 'Python_(programming_language)', 'Matlab': 'MATLAB', 'R': 'R_(programming_language)', \n",
    "             'Java':'Java_(programming_language)', 'SQL': 'SQL', 'JavaScript':'JavaScript'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Java \n",
      "\n",
      "[(u'java', 122), (u'class', 36), (u'method', 27), (u'sun', 19), (u'implementation', 16), (u'memory', 15), (u'public', 15)]\n",
      "\n",
      "Python \n",
      "\n",
      "[(u'python', 75), (u'language', 21), (u'programming', 19), (u'standard', 15), (u'c', 13), (u'languages', 11), (u\"python's\", 10)]\n",
      "\n",
      "JavaScript \n",
      "\n",
      "[(u'javascript', 85), (u'web', 32), (u'code', 19), (u'language', 18), (u'browsers', 16), (u'browser', 15), (u'netscape', 14)]\n",
      "\n",
      "R \n",
      "\n",
      "[(u'r', 40), (u'data', 15), (u'statistical', 11), (u'r,', 7), (u'packages,', 6), (u'programming', 6), (u'packages', 6)]\n",
      "\n",
      "Matlab \n",
      "\n",
      "[(u'matlab', 39), (u'function', 10), (u'other', 9), (u'value', 8), (u'programming', 7), (u'functions', 7), (u'array', 6)]\n",
      "\n",
      "SQL \n",
      "\n",
      "[(u'sql', 58), (u'data', 25), (u'relational', 18), (u'standard', 16), (u'query', 13), (u'value', 12), (u'database', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each of the languages in the dictionary and pull some data, do some basica analyses\n",
    "for language in languages:\n",
    "    \n",
    "    ## Get the website data\n",
    "    # Get the full URL for languages wiki page\n",
    "    language_url = wiki_url + languages[language]\n",
    "    # Request the webpage\n",
    "    language_wikipage = requests.get(language_url)\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    language_wikipage_soup = BeautifulSoup(language_wikipage.content, 'lxml')\n",
    "    \n",
    "    ### Do some analyses\n",
    "    \n",
    "    ## We could get all the links from the pages, like we did before, to analyze if we want\n",
    "    # Initialize lists to store the links\n",
    "    internal_links = list()\n",
    "    citations = list()\n",
    "    external_links = list()\n",
    "    other_links = list()\n",
    "    # Loop through all the links on the page\n",
    "    for link in language_wikipage_soup.find_all('a'):\n",
    "        # Only examine link if it has a 'href', meaning it actually links somewhere\n",
    "        if link.get('href'):\n",
    "            # If 'wiki' is in the href, it points to a wikipedia page. Collect these.\n",
    "            if 'wiki' in link.get('href'):\n",
    "                internal_links.append(link.get('href'))\n",
    "            # If 'cite' is in the href, it is a citation link. Collect these. \n",
    "            elif 'cite' in link.get('href'):\n",
    "                citations.append(link.get('href'))\n",
    "            # If 'http' is in the href, it is some kind of external link. Collect these. \n",
    "            elif 'http' in link.get('href'):\n",
    "                external_links.append(link.get('href'))\n",
    "            # Collect any other links\n",
    "            else:\n",
    "                other_links.append(link.get('href'))\n",
    "            \n",
    "    # Get counts of each of the different link types\n",
    "    n_links_external = size(external_links)\n",
    "    n_links_internal = size(internal_links)\n",
    "    n_citations = size(citations)\n",
    "    n_other_links = size(other_links)\n",
    "    \n",
    "    ## Analyze some of the text\n",
    "    # Get a list of all the paragraph tags in our web page\n",
    "    language_wiki_paragraphs = language_wikipage_soup.find_all('p')\n",
    "    # Loop through all the paragraphs, collecting the text contained inside them. \n",
    "    all_text = ''\n",
    "    for p in language_wiki_paragraphs:\n",
    "        all_text = all_text + p.get_text()\n",
    "    # Split up the words\n",
    "    words = all_text.split()\n",
    "    \n",
    "    # Remove uninteresting words\n",
    "    bad_words = ('a', 'by', 'from', 'which', 'such', 'has', 'also', 'to', 'and', 'be', 'into', 'the', 'you', 'will', \n",
    "                 'this', 'have', 'use', 'of', 'is', 'in', 'that', 'can', 'it', 'or', 'as', 'for', 'an', 'but', 'not', \n",
    "                 'all', 'on', 'are', 'with', 'at', 'was', 'its', 'than', 'been', 'used', 'using', 'through')\n",
    "    words_cleaned = list()\n",
    "    for word in words:\n",
    "        if word.lower() not in bad_words:\n",
    "            words_cleaned.append(word.lower())\n",
    "    \n",
    "    # Calculate word frequency\n",
    "    word_freq = FreqDist(words_cleaned)\n",
    "    \n",
    "    ## Print out Results\n",
    "    print '\\n', language, '\\n'\n",
    "    \n",
    "    print word_freq.most_common(7)\n",
    "\n",
    "# Note we didn't print all the things we extracted, and we only extracted a small part of the pages we pulled. \n",
    "# There is a lot more we could do here, for example, comparing some how well linked (internally and externally) \n",
    "# the different languages are. Could a better linked wiki page be indicative of a better supported language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyses we ran above are toy analyses (they aren't going to tell us anything amazing), but if you know something about these languages, some of this might look reasonable:\n",
    "- R is a language known for statistical analysis of data\n",
    "- Java calls it's functions 'methods', has classes etc.\n",
    "- JavaScript is often used on the web / in browsers\n",
    "- SQL is a databasing langauge, with tools to query the database\n",
    "- and so on...\n",
    "\n",
    "There are a lot more programming languages out there. Maybe we could cluster there main uses and attributes by doing a this kind of web crawling and text analysis at a bigger scale? We could use something like word clouds to visualize the data. Or, if we stumble upon a new language, maybe the quickest way to get a sense of what it does is to run this on it's wiki page. \n",
    "\n",
    "\n",
    "The basic tools to get data and start looking at it are here, but we've skipped quite a lot of important steps \n",
    "Things to update for a real analyses:\n",
    "- Saving the web data:\n",
    "    - We probably don't want to scrape and analyze all together, but rather scrape the data, save it nicely, and then analyze.\n",
    "- Data cleaning:\n",
    "    - Ensuring the quality of the data is really important, something we did not do here. Notice that in some of our most popular words, having punctuation is causing duplicate words, and that is not how we want it to work. \n",
    "- Doing a better (real) analyses, in this case probably some kind of textual analysis:\n",
    "    - For example, what if we did a sentiment analysis on the wiki pages, and tried to predict language popularity?\n",
    "- Getting other data:\n",
    "    - Here we just scraped text from websites. In practice, you might want to scrape other types of data, for example, polling a website periodically to extract something like stock data. Or pulling pictures to train your computer vision algortihm with. Or monitoring websites to send you an alert when something changes. You might also be visiting websites that the scraper would have to log into with some kind of credentials that you prodive the script with. There is a lot more one could do with web scraping, but at it's core calling a website, getting the data, and parsing the parts you need like we did here is the basis of getting data from the web. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
